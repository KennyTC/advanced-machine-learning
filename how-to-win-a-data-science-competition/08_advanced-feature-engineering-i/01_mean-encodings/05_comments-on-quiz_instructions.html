<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Question 1
 </h1>
 <h2 level="2">
  What can be an indicator of usefulness of mean encodings?
 </h2>
 <p>
  <strong>
   Correct answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     Categorical variables with lots of levels.
    </u>
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Incorrect answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     A lot of binary variables.
    </u>
    This is not an indicator because the majority of ML models deal with binary variables just fine. But keep in mind that there could be special cases when encoding binary variables with target mean may be useful. For example, KNN models might work better on mean-encoded binary features.
    <u>
    </u>
   </p>
  </li>
  <li>
   <p>
    <u>
     Learning to rank task.
    </u>
    There is no connection between mean encodings and learning to rank tasks.
   </p>
  </li>
 </ul>
 <h1 level="1">
  Question 2
 </h1>
 <h2 level="2">
  What is the purpose of regularization in case of mean encodings?
 </h2>
 <p>
  <strong>
   Correct answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     Regularization reduces target variable leakage during the construction of mean encodings.
    </u>
   </p>
  </li>
  <li>
   <p>
    <u>
     Regularization allows us to better utilize mean encodings.
    </u>
    Only with regularization we can use mean encodings to the fullest.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Incorrect answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     Regularization allows to make feature space more sparse.
    </u>
    Don't mix it up with L1 penalty.
   </p>
  </li>
 </ul>
 <h1 level="1">
  Question 3
 </h1>
 <h2 level="2">
  What is the correct way of validation when doing mean encodings?
 </h2>
 <p>
  <strong>
   Correct answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     First split the data into train and validation, then estimate encodings on train, then apply them to validation, then validate the model on that split.
    </u>
    That way we avoid target variable leakage.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Incorrect answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     Fix cross-validation split, use that split to calculate mean encodings with CV-loop regularization, use the same split to validate the model.
    </u>
    This way we will overfit, because target from validation is used to calculate mean encodings on train. So the model implicitly uses this information which results in mild target leakage.
   </p>
  </li>
  <li>
   <p>
    <u>
     Calculate mean encodings on all train data, regularize them, then validate your model on random validation split.
    </u>
    This way we will overfit even more, because target from validation is explicitly used to calculate mean encodings.
   </p>
  </li>
 </ul>
 <h1 level="1">
  Question 4
 </h1>
 <h2 level="2">
  Suppose we have a data frame 'df' with categorical variable 'item_id' and target variable 'target'.We create 2 different mean encodings:
 </h2>
 <h2 level="2">
  1)via df['item_id_encoded1'] = df.groupby('item_id')['target'].transform('mean')
 </h2>
 <h2 level="2">
  2)via OneHotEncoding item_id, fitting Linear Regression on one hot-encoded version of item_id and then calculating 'item_id_encoded2' as a prediction from this linear regression on the same data.
 </h2>
 <p>
  <strong>
   Correct answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     'item_id_encoded1' and 'item_id_encoded2' will be essentially the same only if linear regression was fitted without a regularization.
    </u>
    Remember, after one hot encoding there will be only one '1' in each row and the rest - zeros. Since we don't have a regularization, coefficients of each variable would be target means of item_id corresponding to that variable.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Incorrect answers:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <u>
     'item_id_encoded1' and 'item_id_encoded2' will be essentially the same.
    </u>
    Thats not always true. With regularization it will not converge to the same values as 'item_id_encoded1'
   </p>
  </li>
  <li>
   <p>
    <u>
     'item_id_encoded1' and 'item_id_encoded2' may hugely vary due to rare categories.
    </u>
    No, it has nothing to do with categorical sizes.
   </p>
  </li>
 </ul>
 <p>
 </p>
 <p>
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
